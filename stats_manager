# core/stats_manager.py

import time
import logging
import json
import os
from dataclasses import dataclass, field
from typing import Dict, Any, Optional
from collections import deque

logger = logging.getLogger(__name__)

def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """Carrega configuração dos modelos a partir do arquivo JSON."""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Arquivo {config_path} não encontrado. Este arquivo é obrigatório.")
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            models_config = json.load(f)
        if not models_config:
            raise ValueError(f"Arquivo {config_path} está vazio ou inválido.")
        return models_config
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON em {config_path}: {e}")
    except Exception as e:
        raise RuntimeError(f"Erro ao carregar {config_path}: {e}")

# Carrega configuração dos modelos uma vez no init do módulo
MODELS_CONFIG = load_models_config()

@dataclass
class Stats:
    """Classe centralizada para stats de batch ou global, incluindo calibração, rate limiting e métricas refinadas."""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_tokens_input: int = 0
    total_tokens_output: int = 0
    total_tokens_cached: int = 0
    total_cost: float = 0.0
    processing_time: float = 0.0
    start_time: float = field(default_factory=time.time)
    total_api_response_time: float = 0.0
    total_wait_time: float = 0.0
    total_retry_time: float = 0.0
    min_response_time: float = float('inf')
    max_response_time: float = 0.0
    retry_attempts: int = 0
    concurrent_peak: int = 0
    errors_by_type: Dict[str, int] = field(default_factory=dict)
    api_rate_limits_detected: int = 0
    prevented_rate_limits: int = 0
    coordinated_wait_time: float = 0.0
    cost_breakdown: Dict[str, float] = field(default_factory=dict)
    model_used: str = ""
    # Métricas centralizadas de calibração
    calibration_factor: float = 1.0
    calibration_accuracy: float = 0.0
    total_calibrations: int = 0
    usage_history: deque = field(default_factory=lambda: deque(maxlen=1000))

    @property
    def total_tokens(self) -> int:
        return self.total_tokens_input + self.total_tokens_output + self.total_tokens_cached

    @property
    def success_rate(self) -> float:
        return (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0.0

    @property
    def avg_rate(self) -> float:
        return (self.successful_requests / self.processing_time) if self.processing_time > 0 else 0.0

    @property
    def avg_response_time(self) -> float:
        return (self.total_api_response_time / self.successful_requests) if self.successful_requests > 0 else 0.0

    @property
    def efficiency_rate(self) -> float:
        total_time = self.processing_time
        total_wait = self.total_wait_time + self.coordinated_wait_time + self.total_retry_time
        processing_time = max(0, total_time - total_wait)
        return (processing_time / total_time * 100) if total_time > 0 else 0.0

    @property
    def cost_per_token(self) -> float:
        return (self.total_cost / self.total_tokens) if self.total_tokens > 0 else 0.0

    @property
    def cache_hit_rate(self) -> float:
        total_input_and_cached = self.total_tokens_input + self.total_tokens_cached
        return (self.total_tokens_cached / total_input_and_cached * 100) if total_input_and_cached > 0 else 0.0

class StatsManager:
    """
    Gerenciador centralizado de stats, incluindo calibração, rate limiting e cálculos de custo.
    - Integra dados do rate_limiter para consistência.
    - Suporta adaptação dinâmica via warm-up e ETAs.
    - Centraliza todo cálculo de pricing aqui, carregando de models.json para evitar duplicação.
    """

    def __init__(self):
        self.global_stats = Stats()
        self._batch_snapshots: Dict[str, Dict[str, Any]] = {}
        self._current_concurrent = 0
        self._peak_concurrent_ever = 0
        self._max_concurrent_limit = 10

    def set_max_concurrent(self, max_concurrent: int) -> None:
        self._max_concurrent_limit = max_concurrent

    def start_batch(self, batch_id: str) -> None:
        self._batch_snapshots[batch_id] = {
            'start_time': time.time(),
            'batch_peak_concurrent': 0,
            'start_stats': Stats(**vars(self.global_stats))  # Cópia profunda
        }

    def end_batch(self, batch_id: str) -> Stats:
        if batch_id not in self._batch_snapshots:
            raise ValueError(f"Batch {batch_id} não iniciado")
        snapshot = self._batch_snapshots.pop(batch_id)
        batch_stats = Stats()
        for field in vars(self.global_stats):
            setattr(batch_stats, field, getattr(self.global_stats, field) - getattr(snapshot['start_stats'], field))
        batch_stats.processing_time = time.time() - snapshot['start_time']
        batch_stats.concurrent_peak = min(snapshot['batch_peak_concurrent'], self._max_concurrent_limit)
        return batch_stats

    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int, cached_tokens: int = 0) -> float:
        """Cálculo centralizado de custo, usando preços carregados de models.json."""
        if model not in MODELS_CONFIG:
            logger.warning(f"Modelo '{model}' não encontrado em models.json. Usando preços fallback.")
            pricing = {'input': 0.03, 'output': 0.06, 'cache': 0.015}  # Fallback
        else:
            model_data = MODELS_CONFIG[model]
            pricing = {
                'input': model_data.get('input', 0.03),
                'output': model_data.get('output', 0.06),
                'cache': model_data.get('cache', 0.015)
            }
        regular_input_tokens = max(0, input_tokens - cached_tokens)
        regular_input_cost = (regular_input_tokens / 1000) * pricing['input']
        cached_cost = (cached_tokens / 1000) * pricing['cache']
        output_cost = (output_tokens / 1000) * pricing['output']
        total_cost = regular_input_cost + cached_cost + output_cost
        if cached_tokens > 0:
            logger.debug(
                f"Cálculo de custo: Modelo {model} | Input regular: {regular_input_tokens} (${regular_input_cost:.6f}) | Cached: {cached_tokens} (${cached_cost:.6f}) | Output: {output_tokens} (${output_cost:.6f}) | Total: ${total_cost:.6f}",
                extra={'model': model, 'total_cost': total_cost, 'action': 'cost_calculation'}
            )
        return total_cost

    def record_request(self, success: bool, tokens_input: int = 0, tokens_output: int = 0, tokens_cached: int = 0,
                       api_response_time: float = 0.0, wait_time: float = 0.0, retry_time: float = 0.0,
                       error_type: Optional[str] = None, retry_count: int = 0, model: str = "") -> None:
        """Registra requisição com custos e métricas refinadas (inclui separação de tempos)."""
        self.global_stats.total_requests += 1
        cost = self.calculate_cost(model, tokens_input, tokens_output, tokens_cached)
        if success:
            self.global_stats.successful_requests += 1
            self.global_stats.total_api_response_time += api_response_time
            self.global_stats.min_response_time = min(self.global_stats.min_response_time, api_response_time)
            self.global_stats.max_response_time = max(self.global_stats.max_response_time, api_response_time)
        else:
            self.global_stats.failed_requests += 1
            if error_type:
                self.global_stats.errors_by_type[error_type] = self.global_stats.errors_by_type.get(error_type, 0) + 1
        self.global_stats.total_tokens_input += tokens_input
        self.global_stats.total_tokens_output += tokens_output
        self.global_stats.total_tokens_cached += tokens_cached
        self.global_stats.total_cost += cost
        self.global_stats.total_wait_time += wait_time
        self.global_stats.total_retry_time += retry_time
        self.global_stats.retry_attempts += retry_count
        if model:
            self.global_stats.model_used = model
            self.global_stats.cost_breakdown[model] = self.global_stats.cost_breakdown.get(model, 0) + cost

    def record_rate_limit_wait(self, wait_time: float) -> None:
        self.global_stats.total_wait_time += wait_time
        self.global_stats.prevented_rate_limits += 1

    def record_api_rate_limit(self) -> None:
        self.global_stats.api_rate_limits_detected += 1

    def record_token_usage_for_calibration(self, estimated: int, actual: int) -> None:
        """Centraliza calibração com precisão e weights."""
        if actual <= 0 or estimated <= 0:
            return
        ratio = actual / estimated
        self.global_stats.usage_history.append({'estimated': estimated, 'actual': actual, 'ratio': ratio})
        self.global_stats.total_calibrations += 1
        if 0.8 <= ratio <= 1.2:
            self.global_stats.calibration_accuracy = (self.global_stats.calibration_accuracy * (self.global_stats.total_calibrations - 1) + 1) / self.global_stats.total_calibrations
        else:
            self.global_stats.calibration_accuracy = (self.global_stats.calibration_accuracy * (self.global_stats.total_calibrations - 1) + 0) / self.global_stats.total_calibrations
        if len(self.global_stats.usage_history) >= 20 and time.time() - self.global_stats.start_time > 60:
            self._recalibrate()

    def _recalibrate(self) -> None:
        """Recalibração centralizada com weights priorizando recentes."""
        history = list(self.global_stats.usage_history)[-200:]
        total_weight = sum((i + 1) for i in range(len(history)))
        weighted_sum = sum(entry['ratio'] * (i + 1) for i, entry in enumerate(history))
        avg_ratio = weighted_sum / total_weight if total_weight > 0 else 1.0
        self.global_stats.calibration_factor = max(0.3, min(3.0, (avg_ratio * 0.6) + (self.global_stats.calibration_factor * 0.4)))

    def record_concurrent_start(self) -> None:
        if self._current_concurrent < self._max_concurrent_limit:
            self._current_concurrent += 1
        self._peak_concurrent_ever = max(self._peak_concurrent_ever, self._current_concurrent)
        self.global_stats.concurrent_peak = self._peak_concurrent_ever
        for snapshot in self._batch_snapshots.values():
            snapshot['batch_peak_concurrent'] = max(snapshot['batch_peak_concurrent'], self._current_concurrent)

    def record_concurrent_end(self) -> None:
        self._current_concurrent = max(0, self._current_concurrent - 1)

    def get_global_stats(self) -> Stats:
        self.global_stats.processing_time = time.time() - self.global_stats.start_time
        return self.global_stats

    def format_stats(self, stats: Stats, title: str = "Stats") -> str:
        """Formatação otimizada e concisa para output em notebooks."""
        lines = [f"--- {title} ---"]
        lines.append(f"Total Requests: {stats.total_requests}")
        lines.append(f"Successful: {stats.successful_requests} ({stats.success_rate:.2f}%)")
        lines.append(f"Failed: {stats.failed_requests}")
        lines.append(f"Total Tokens: {stats.total_tokens} (Input: {stats.total_tokens_input}, Output: {stats.total_tokens_output}, Cached: {stats.total_tokens_cached})")
        lines.append(f"Cache Hit Rate: {stats.cache_hit_rate:.2f}%")
        lines.append(f"Total Cost: ${stats.total_cost:.6f} (Per Token: ${stats.cost_per_token:.8f})")
        lines.append(f"Processing Time: {stats.processing_time:.2f}s (Efficiency: {stats.efficiency_rate:.2f}%)")
        lines.append(f"Avg Response Time: {stats.avg_response_time:.2f}s (Min: {stats.min_response_time:.2f}s, Max: {stats.max_response_time:.2f}s)")
        lines.append(f"Total Wait Time: {stats.total_wait_time:.2f}s | Total Retry Time: {stats.total_retry_time:.2f}s")
        lines.append(f"Retry Attempts: {stats.retry_attempts}")
        lines.append(f"Concurrent Peak: {stats.concurrent_peak}")
        lines.append(f"Rate Limits Detected: {stats.api_rate_limits_detected} | Prevented: {stats.prevented_rate_limits}")
        lines.append(f"Calibration Factor: {stats.calibration_factor:.3f} | Accuracy: {stats.calibration_accuracy:.2f}")
        if stats.errors_by_type:
            lines.append("Errors by Type:")
            for err_type, count in stats.errors_by_type.items():
                lines.append(f"  - {err_type}: {count}")
        if stats.cost_breakdown:
            lines.append("Cost Breakdown by Model:")
            for model, cost in stats.cost_breakdown.items():
                lines.append(f"  - {model}: ${cost:.6f}")
        return "\n".join(lines)
