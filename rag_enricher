# core/rag_enricher.py

import os
import logging
import hashlib
import json
from typing import List, Dict, Optional, Tuple
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import words
import re
from transformers import pipeline

nltk.download('punkt', quiet=True)
nltk.download('words', quiet=True)

logger = logging.getLogger(__name__)

class RAGEnricher:
    """
    RAGEnricher atualizado: Enriches prompts with contextual examples from a local vector database.
    - Suporta limpeza, correção contextual para transcrições incoerentes e chunking para textos longos.
    - Atualiza DB localmente se arquivos mudarem.
    """

    def __init__(
        self,
        folder_path: str,
        db_path: str = "./rag_db",
        embedding_model: str = "paraphrase-mpnet-base-v2",  # Robusto a ruído
        collection_name: str = "examples",
        force_rebuild: bool = False,
        correction_level: str = "medium",  # 'low', 'medium', 'high' for incoherence handling
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        use_summary: bool = False
    ):
        self.folder_path = folder_path
        self.db_path = db_path
        self.embedding_model = SentenceTransformer(embedding_model)
        self.client = chromadb.PersistentClient(path=db_path, settings=Settings(allow_reset=True))
        self.collection = self.client.get_or_create_collection(name=collection_name)
        self.metadata_file = os.path.join(db_path, "file_metadata.json")
        self.file_metadata = self._load_metadata()
        self.correction_level = correction_level
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_summary = use_summary
        self.corrector = pipeline("text2text-generation", model="t5-small") if correction_level != 'low' else None
        if force_rebuild:
            self.client.reset()
            self.file_metadata = {}
        self._build_or_update_db()

    def _load_metadata(self) -> Dict[str, str]:
        if os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        return {}

    def _save_metadata(self):
        with open(self.metadata_file, 'w') as f:
            json.dump(self.file_metadata, f)

    def _get_file_hash(self, file_path: str) -> str:
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            hasher.update(f.read())
        return hasher.hexdigest()

    def _correct_incoherent_text(self, text: str) -> str:
        """Corrige frases incoerentes usando modelo de reescrita."""
        if self.correction_level == 'low':
            return text
        sentences = sent_tokenize(text)
        corrected = []
        for sentence in sentences:
            if len(sentence.split()) < 3:  # Skip very short
                corrected.append(sentence)
                continue
            generated = self.corrector(f"rephrase: {sentence}", max_length=128, num_return_sequences=1)[0]['generated_text']
            corrected.append(generated)
        return ' '.join(corrected)

    def _clean_text(self, text: str) -> str:
        text = re.sub(r'\s+', ' ', text).strip()
        text = self._correct_incoherent_text(text)
        return text

    def _chunk_text(self, text: str) -> List[str]:
        sentences = sent_tokenize(text)
        chunks = []
        current_chunk = []
        current_length = 0
        for sentence in sentences:
            sent_len = len(sentence.split())
            if current_length + sent_len > self.chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else current_chunk
                current_length = sum(len(s.split()) for s in current_chunk)
            current_chunk.append(sentence)
            current_length += sent_len
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        return chunks

    def _generate_summary(self, text: str) -> str:
        return self.corrector(f"summarize: {text}", max_length=100, num_return_sequences=1)[0]['generated_text']

    def _build_or_update_db(self):
        files = [f for f in os.listdir(self.folder_path) if f.endswith('.txt')]
        if not files:
            logger.warning("No files found in folder. DB not updated.")
            return

        updated_count = 0
        for file in files:
            file_path = os.path.join(self.folder_path, file)
            current_hash = self._get_file_hash(file_path)
            stored_hash = self.file_metadata.get(file)

            if current_hash != stored_hash:
                with open(file_path, 'r') as f:
                    raw_content = f.read()
                cleaned_content = self._clean_text(raw_content)
                
                chunks = self._chunk_text(cleaned_content)
                embeddings = []
                documents = []
                metadatas = []
                ids = []
                
                for i, chunk in enumerate(chunks):
                    doc_id = f"{file}_chunk_{i}"
                    embedding = self.embedding_model.encode(chunk).tolist()
                    metadata = {"file": file, "hash": current_hash, "chunk": i}
                    if self.use_summary and i == 0:
                        summary = self._generate_summary(cleaned_content)
                        summary_embedding = self.embedding_model.encode(summary).tolist()
                        self.collection.upsert(
                            ids=[f"{file}_summary"],
                            embeddings=[summary_embedding],
                            documents=[summary],
                            metadatas=[{"file": file, "type": "summary"}]
                        )
                    embeddings.append(embedding)
                    documents.append(chunk)
                    metadatas.append(metadata)
                    ids.append(doc_id)
                
                self.collection.upsert(
                    ids=ids,
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas
                )
                self.file_metadata[file] = current_hash
                updated_count += 1

        self._save_metadata()
        if updated_count > 0:
            logger.info(f"Updated {updated_count} files in vector DB.")
        else:
            logger.info("No changes detected; DB up-to-date.")

    def retrieve_examples(
        self,
        query_text: str,
        top_k: int = 5,
        min_similarity: float = 0.5
    ) -> List[Dict[str, any]]:
        cleaned_query = self._clean_text(query_text)
        query_embedding = self.embedding_model.encode(cleaned_query).tolist()
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents", "distances", "metadatas"]
        )

        filtered_results = []
        for doc, dist, meta in zip(results['documents'][0], results['distances'][0], results['metadatas'][0]):
            similarity = 1 - dist
            if similarity >= min_similarity:
                cleaned_doc = self._clean_text(doc)
                re_embedding = self.embedding_model.encode(cleaned_doc).tolist()
                re_similarity = 1 - self.embedding_model.cosine_distances([query_embedding], [re_embedding])[0][0]
                if re_similarity >= min_similarity:
                    filtered_results.append({
                        "content": doc,
                        "similarity": similarity,
                        "metadata": meta
                    })

        if not filtered_results:
            logger.info("No examples met the similarity threshold after validation.")
        return filtered_results

    def enrich_prompt(
        self,
        input_text: str,
        prompt_template: str,
        top_k: int = 5,
        min_similarity: float = 0.5,
        confidence_threshold: float = 0.8,
        max_examples_in_prompt: int = 3
    ) -> Tuple[str, bool, List[Dict[str, any]]]:
        examples = self.retrieve_examples(input_text, top_k, min_similarity)
        if not examples:
            return prompt_template.format(text=input_text, context=""), True, []

        top_similarity = examples[0]["similarity"]
        should_process_with_ai = top_similarity < confidence_threshold

        limited_examples = examples[:max_examples_in_prompt]
        context = "\n".join([ex["content"] for ex in limited_examples])
        enriched = prompt_template.format(text=input_text, context=context)

        return enriched, should_process_with_ai, limited_examples

    def process_with_confidence(
        self,
        texts: List[str],
        prompt_template: str,
        ai_processor: Optional[any] = None,
        **kwargs
    ) -> List[Dict[str, any]]:
        results = []
        for text in texts:
            enriched, should_ai, examples = self.enrich_prompt(text, prompt_template, **kwargs)
            if not should_ai:
                results.append({
                    "text": text,
                    "classification": "High confidence vector match",
                    "examples": examples
                })
            elif ai_processor:
                ai_result = asyncio.run(ai_processor.process_single(text, enriched, **kwargs))
                results.append({
                    "text": text,
                    "ai_result": ai_result,
                    "examples": examples
                })
            else:
                results.append({
                    "text": text,
                    "enriched_prompt": enriched,
                    "examples": examples
                })
        return results
