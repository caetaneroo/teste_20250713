import asyncio
import time
import json
import logging
import re
import os
from typing import List, Dict, Any, Optional, Callable

from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI  # Substitua pelo seu cliente de API real

from .rate_limiter import AdaptiveRateLimiter
from .stats_manager import StatsManager

logger = logging.getLogger(__name__)

# --- Constantes de ConfiguraÃ§Ã£o ---
MAX_RETRY = 3  # Aumentado para 3 para dar mais uma chance em caso de falhas transitÃ³rias

def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """
    Carrega a configuraÃ§Ã£o dos modelos a partir de um arquivo JSON.
    Este arquivo Ã© a Ãºnica fonte da verdade para modelos, preÃ§os e capacidades.
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Arquivo de configuraÃ§Ã£o '{config_path}' nÃ£o encontrado. Este arquivo Ã© obrigatÃ³rio.")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            models_config = json.load(f)
        
        if not models_config or not isinstance(models_config, dict):
            raise ValueError(f"Arquivo '{config_path}' estÃ¡ vazio ou nÃ£o Ã© um dicionÃ¡rio JSON vÃ¡lido.")
        
        return models_config
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON em '{config_path}': {e}")
    except Exception as e:
        raise RuntimeError(f"Erro inesperado ao carregar '{config_path}': {e}")

class JSONSchemaNotSupportedError(Exception):
    """ExceÃ§Ã£o levantada quando json_schema Ã© usado com um modelo incompatÃ­vel."""
    pass

class AIProcessor:
    """
    Orquestrador de chamadas para a API de IA, focado em alta eficiÃªncia e robustez.

    Esta classe encapsula a lÃ³gica de interaÃ§Ã£o com a API, delega o controle de
    fluxo para o `AdaptiveRateLimiter` e o gerenciamento de mÃ©tricas para o
    `StatsManager`, resultando em um sistema modular e de alta performance.
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o processador de IA.

        Args:
            config (Dict[str, Any]): DicionÃ¡rio de configuraÃ§Ã£o contendo:
                - client_id, client_secret: Credenciais da API.
                - model: O modelo de IA a ser usado (e.g., 'gpt-4o-mini').
                - max_tpm: Limite de Tokens Por Minuto.
                - Outros parÃ¢metros opcionais como temperature, max_tokens, etc.
        """
        # Carrega a configuraÃ§Ã£o dos modelos, que serÃ¡ a fonte da verdade
        self.models_config = load_models_config()
        self.supported_models = set(self.models_config.keys())
        
        # --- ConfiguraÃ§Ã£o Essencial ---
        self.client_id = config['client_id']
        self.client_secret = config['client_secret']
        self.model = config.get('model', 'gpt-4o-mini')

        if self.model not in self.supported_models:
            raise ValueError(
                f"Modelo '{self.model}' nÃ£o encontrado em models.json. "
                f"Modelos suportados: {', '.join(sorted(self.supported_models))}"
            )

        # --- ParÃ¢metros da API ---
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens')
        
        # --- InicializaÃ§Ã£o dos MÃ³dulos de Suporte ---
        self.stats_manager = StatsManager(models_config=self.models_config)
        
        # O RateLimiter recebe um callback para notificar o StatsManager sobre eventos
        self.rate_limiter = AdaptiveRateLimiter(
            max_tpm=config['max_tpm'],
            stats_callback=self._create_stats_callback()
        )

        # --- Cliente da API ---
        self.client = AsyncIaraGenAI(
            client_id=self.client_id,
            client_secret=self.client_secret,
            environment=config.get('environment', 'dev'),
            provider=config.get('provider', 'azure_openai'),
            correlation_id=config.get('correlation_id', 'ai-processor-v2')
        )
        
        logger.info(
            "AIProcessor inicializado com sucesso",
            extra={
                'action': 'ai_processor_init',
                'model': self.model,
                'max_tpm': config['max_tpm'],
                'json_schema_supported': self._supports_json_schema(self.model)
            }
        )

    def _create_stats_callback(self) -> Callable:
        """Cria uma funÃ§Ã£o de callback para o RateLimiter se comunicar com o StatsManager."""
        def callback(event: Dict[str, Any], batch_id: Optional[str] = None):
            # Envia o evento para o StatsManager em uma nova tarefa para nÃ£o bloquear o RateLimiter
            asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
        return callback

    def _supports_json_schema(self, model: str) -> bool:
        """Verifica se o modelo suporta json_schema, usando models.json como fonte."""
        return self.models_config.get(model, {}).get('json_schema', False)

    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]]):
        """Valida se o modelo atual suporta json_schema antes de prosseguir."""
        if json_schema and not self._supports_json_schema(self.model):
            supported_models_str = ', '.join(
                m for m, cfg in self.models_config.items() if cfg.get('json_schema')
            )
            raise JSONSchemaNotSupportedError(
                f"O modelo '{self.model}' nÃ£o suporta json_schema. "
                f"Modelos compatÃ­veis: {supported_models_str}. "
                "Considere usar um modelo compatÃ­vel ou remover o json_schema."
            )
    
    @staticmethod
    def _is_rate_limit_error(result: Dict[str, Any]) -> bool:
        """Verifica se um resultado de erro Ã© um erro de rate limit."""
        if not isinstance(result, dict):
            return False
        error_msg = result.get('error', '').lower()
        return 'token rate limit' in error_msg or 'rate limit' in error_msg

    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        """Extrai o tempo de espera de uma mensagem de erro de rate limit."""
        # Prioriza o header 'retry-after', que geralmente Ã© mais confiÃ¡vel
        headers = result.get('response_headers', {})
        if 'retry-after' in headers:
            try:
                return float(headers['retry-after'])
            except (ValueError, TypeError):
                pass

        error_msg = result.get('error', '').lower()
        match = re.search(r'try again in.*?(\d+\.?\d*)\s*s', error_msg)
        if match:
            return float(match.group(1))

        # Fallback para um valor padrÃ£o seguro
        return 60.0

    async def _make_api_call(
        self,
        messages: List[Dict[str, str]],
        json_schema: Optional[Dict[str, Any]],
        request_id: str,
        batch_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Realiza a chamada Ã  API, com retries e tratamento de erros.
        Este Ã© o coraÃ§Ã£o da interaÃ§Ã£o com a API.
        """
        self._validate_json_schema_compatibility(json_schema)
        
        # Delega ao RateLimiter a decisÃ£o de quando prosseguir
        await self.rate_limiter.await_permission_to_proceed()

        await self.stats_manager.record_concurrent_start(batch_id)
        
        try:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(MAX_RETRY), wait=wait_fixed(1), reraise=True):
                with attempt:
                    start_time = time.time()
                    try:
                        api_params = {
                            "model": self.model,
                            "messages": messages,
                            "temperature": self.temperature,
                        }
                        if self.max_tokens:
                            api_params["max_tokens"] = self.max_tokens
                        if json_schema:
                            api_params["response_format"] = {"type": "json_object"} # Assume modo JSON

                        response = await self.client.chat.completions.create(**api_params)
                        end_time = time.time()

                        # Parse do conteÃºdo
                        content = response.choices[0].message.content
                        parsed_content = content
                        if json_schema and content:
                            try:
                                parsed_content = json.loads(content)
                            except json.JSONDecodeError:
                                # A resposta nÃ£o foi um JSON vÃ¡lido, mas a chamada foi um sucesso
                                # A lÃ³gica de validaÃ§Ã£o do resultado deve tratar isso
                                pass
                        
                        return {
                            'id': request_id,
                            'success': True,
                            'content': parsed_content,
                            'raw_content': content,
                            'input_tokens': response.usage.prompt_tokens,
                            'output_tokens': response.usage.completion_tokens,
                            'tokens_used': response.usage.total_tokens,
                            'cached_tokens': getattr(getattr(response.usage, 'prompt_tokens_details', {}), 'cached_tokens', 0),
                            'api_response_time': end_time - start_time,
                            'attempts': attempt.retry_state.attempt_number,
                        }

                    except Exception as e:
                        end_time = time.time()
                        error_result = {
                            'id': request_id,
                            'success': False,
                            'error': str(e),
                            'error_type': type(e).__name__,
                            'api_response_time': end_time - start_time,
                            'attempts': attempt.retry_state.attempt_number,
                            'response_headers': dict(getattr(e, 'response', {}).headers or {}),
                        }
                        # Se for um erro de rate limit, notifica o limiter e levanta a exceÃ§Ã£o
                        # para o tenacity tentar novamente apÃ³s a pausa.
                        if self._is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            self.rate_limiter.record_api_rate_limit(wait_time)
                            # Notifica o StatsManager
                            await self.stats_manager.record_rate_limiter_event({
                                'event_type': 'api_rate_limit_detected',
                                'wait_time': wait_time
                            }, batch_id)
                            raise  # ForÃ§a o retry do Tenacity, que aguardarÃ¡ o wait_fixed
                        
                        # Para outros erros, retorna o resultado imediatamente sem retry
                        return error_result

        except RetryError as retry_error:
            # Ocorre se todas as tentativas falharem
            return {
                'id': request_id,
                'success': False,
                'error': f'MÃ¡ximo de {MAX_RETRY} tentativas excedido. Ãšltimo erro: {retry_error.last_attempt.exception()}',
                'error_type': 'RetryError',
                'attempts': MAX_RETRY,
            }
        finally:
            await self.stats_manager.record_concurrent_end(batch_id)
    
    async def process_single(
        self,
        text: str,
        prompt_template: str,
        json_schema: Optional[Dict[str, Any]] = None,
        custom_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Processa um Ãºnico texto."""
        request_id = custom_id or f"req_{int(time.time() * 1000)}"
        prompt = prompt_template.format(text=text, **kwargs)
        messages = [{"role": "user", "content": prompt}]
        
        result = await self._make_api_call(messages, json_schema, request_id)

        # Informa o RateLimiter sobre o resultado para ajuste dinÃ¢mico
        self.rate_limiter.record_request_completion(
            tokens_used=result.get('tokens_used', 0),
            success=result.get('success', False)
        )
        
        # Registra os dados completos no StatsManager
        await self.stats_manager.record_request(
            model=self.model,
            retry_count=max(0, result.get('attempts', 1) - 1),
            **result
        )
        
        return result

    async def _process_item_in_batch(self, item_data, prompt_template, json_schema, batch_id, **kwargs):
        """FunÃ§Ã£o worker para processar um item de um lote."""
        index, text, custom_id = item_data
        request_id = custom_id or f"{batch_id}_req_{index}"
        
        prompt = prompt_template.format(text=text, **kwargs)
        messages = [{"role": "user", "content": prompt}]
        
        result = await self._make_api_call(messages, json_schema, request_id, batch_id)

        self.rate_limiter.record_request_completion(
            tokens_used=result.get('tokens_used', 0),
            success=result.get('success', False)
        )
        
        await self.stats_manager.record_request(
            batch_id=batch_id,
            model=self.model,
            retry_count=max(0, result.get('attempts', 1) - 1),
            **result
        )

        return result

    async def process_batch(
        self,
        texts: List[str],
        prompt_template: str,
        json_schema: Optional[Dict[str, Any]] = None,
        batch_id: Optional[str] = None,
        custom_ids: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Processa uma lista de textos em lote com alta concorrÃªncia e eficiÃªncia.
        """
        batch_id = batch_id or f"batch_{int(time.time())}"
        self.stats_manager.start_batch(batch_id)

        if custom_ids and len(custom_ids) != len(texts):
            raise ValueError("custom_ids deve ter o mesmo tamanho que texts.")
        
        logger.info(
            f"ğŸš€ Iniciando processamento em lote '{batch_id}' com {len(texts)} itens.",
            extra={'action': 'batch_start', 'batch_id': batch_id, 'total_items': len(texts)}
        )

        # Prepara os dados para cada tarefa
        items_to_process = [
            (i, texts[i], custom_ids[i] if custom_ids else None)
            for i in range(len(texts))
        ]

        # Cria e executa todas as tarefas concorrentemente
        tasks = [
            asyncio.create_task(self._process_item_in_batch(item, prompt_template, json_schema, batch_id, **kwargs))
            for item in items_to_process
        ]
        
        # Aguarda a conclusÃ£o de todas as tarefas
        results = await asyncio.gather(*tasks)
        
        batch_stats = self.stats_manager.end_batch(batch_id)
        
        logger.info(
            f"âœ… Lote '{batch_id}' concluÃ­do. "
            f"Sucesso: {batch_stats.successful_requests}, Falhas: {batch_stats.failed_requests}.",
            extra={
                'action': 'batch_complete',
                'batch_id': batch_id,
                **batch_stats.__dict__ # Loga todas as estatÃ­sticas do lote
            }
        )
        
        return {
            'results': results,
            'batch_stats': batch_stats,
            'batch_id': batch_id
        }

    def get_stats_manager(self) -> StatsManager:
        """Retorna a instÃ¢ncia do StatsManager para anÃ¡lise externa."""
        return self.stats_manager
