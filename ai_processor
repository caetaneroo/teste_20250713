import asyncio
import time
import json
import logging
import re
import os
import traceback
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional, Callable

from tenacity import AsyncRetrying, RetryError, stop_after_attempt, wait_fixed
from iaragenai import AsyncIaraGenAI  # Substitua pelo seu cliente de API real

from rate_limiter import AdaptiveRateLimiter, INITIAL_CONCURRENCY
from stats_manager import StatsManager

logger = logging.getLogger(__name__)

# --- Constantes de ConfiguraÃ§Ã£o ---
MAX_RETRY = 3

def load_models_config(config_path: str = "models.json") -> Dict[str, Any]:
    """
    Carrega a configuraÃ§Ã£o dos modelos a partir de um arquivo JSON.
    Este arquivo Ã© a Ãºnica fonte da verdade para modelos, preÃ§os e capacidades.
    """
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Arquivo de configuraÃ§Ã£o '{config_path}' nÃ£o encontrado. Este arquivo Ã© obrigatÃ³rio.")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            models_config = json.load(f)
        
        if not models_config or not isinstance(models_config, dict):
            raise ValueError(f"Arquivo '{config_path}' estÃ¡ vazio ou nÃ£o Ã© um dicionÃ¡rio JSON vÃ¡lido.")
        
        return models_config
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON em '{config_path}': {e}")
    except Exception as e:
        raise RuntimeError(f"Erro inesperado ao carregar '{config_path}': {e}")

class JSONSchemaNotSupportedError(Exception):
    """ExceÃ§Ã£o levantada quando json_schema Ã© usado com um modelo incompatÃ­vel."""
    pass

class AIProcessor:
    """
    Orquestrador de chamadas para a API de IA, focado em alta eficiÃªncia,
    robustez e visibilidade operacional.
    """

    class _BatchProgressTracker:
        """
        Classe interna para rastrear o progresso de um lote de forma atÃ´mica e
        eficiente, evitando o problema de "event loop starvation".
        """
        def __init__(self, total: int, batch_id: str):
            self.total = total
            self.batch_id = batch_id
            self.completed_count = 0
            self.start_time = time.time()
            self.logged_milestones = set()
            self._lock = asyncio.Lock()

        async def increment_and_log(self):
            """Incrementa o contador e loga se um novo marco de progresso for atingido."""
            async with self._lock:
                self.completed_count += 1
                progress_percent = (self.completed_count / self.total) * 100
                current_milestone = int(progress_percent // 10) * 10
                
                if current_milestone > 0 and current_milestone not in self.logged_milestones:
                    self.logged_milestones.add(current_milestone)
                    elapsed = time.time() - self.start_time
                    rate = self.completed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (self.total - self.completed_count) / rate if rate > 0 else 0
                    
                    logger.info(
                        f"ðŸ“Š Progresso do Lote '{self.batch_id}': {self.completed_count}/{self.total} ({progress_percent:.1f}%) | "
                        f"Taxa: {rate:.2f} reqs/s | ETA: {eta_seconds/60:.1f} min",
                        extra={'action': 'batch_progress', 'batch_id': self.batch_id, 'completed': self.completed_count,
                               'total': self.total, 'progress_percent': round(progress_percent, 1),
                               'processing_rate': round(rate, 2), 'eta_minutes': round(eta_seconds / 60, 1)}
                    )

    def __init__(self, config: Dict[str, Any]):
        self.models_config = load_models_config()
        self.supported_models = set(self.models_config.keys())
        
        self.client_id = config['client_id']
        self.client_secret = config['client_secret']
        self.model = config.get('model', 'gpt-4o-mini')

        if self.model not in self.supported_models:
            raise ValueError(f"Modelo '{self.model}' nÃ£o suportado. Modelos disponÃ­veis: {', '.join(sorted(self.supported_models))}")

        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens')
        
        max_tpm = config.get('max_tpm', 125000)
        initial_concurrency = config.get('initial_concurrency', INITIAL_CONCURRENCY)

        # **CORREÃ‡ÃƒO**: Removido o argumento 'initial_concurrency' da chamada ao construtor do StatsManager.
        self.stats_manager = StatsManager(
            models_config=self.models_config,
            max_tpm=max_tpm
        )
        
        self.rate_limiter = AdaptiveRateLimiter(
            max_tpm=max_tpm,
            stats_callback=self._create_stats_callback(),
            initial_concurrency=initial_concurrency
        )

        self.client = AsyncIaraGenAI(
            client_id=self.client_id,
            client_secret=self.client_secret,
            environment=config.get('environment', 'dev'),
            provider=config.get('provider', 'azure_openai'),
            correlation_id=config.get('correlation_id', 'ai-processor-final')
        )
        
        logger.info("AIProcessor inicializado",
                    extra={'action': 'ai_processor_init', 'model': self.model, 'max_tpm': max_tpm,
                           'json_schema_supported': self._supports_json_schema(self.model)})

    def _get_model_pricing(self, model: str) -> Dict[str, float]:
        model_data = self.models_config.get(model, {})
        return {'input': model_data.get('input', 0.0), 'output': model_data.get('output', 0.0), 'cache': model_data.get('cache', 0.0)}

    def _calculate_cost(self, input_tokens: int, output_tokens: int, cached_tokens: int = 0) -> float:
        pricing = self._get_model_pricing(self.model)
        regular_input = max(0, input_tokens - cached_tokens)
        return ((regular_input / 1000) * pricing['input'] + (cached_tokens / 1000) * pricing['cache'] + (output_tokens / 1000) * pricing['output'])

    def _create_stats_callback(self) -> Callable:
        def callback(event: Dict[str, Any], batch_id: Optional[str] = None):
            asyncio.create_task(self.stats_manager.record_rate_limiter_event(event, batch_id))
        return callback

    def _supports_json_schema(self, model: str) -> bool:
        return self.models_config.get(model, {}).get('json_schema', False)

    def _validate_json_schema_compatibility(self, json_schema: Optional[Dict[str, Any]]):
        if json_schema and not self._supports_json_schema(self.model):
            supported_models = [m for m, cfg in self.models_config.items() if cfg.get('json_schema')]
            raise JSONSchemaNotSupportedError(f"Modelo '{self.model}' nÃ£o suporta json_schema. Modelos compatÃ­veis: {', '.join(supported_models)}")

    @staticmethod
    def _is_rate_limit_error(result: Dict[str, Any]) -> bool:
        if not isinstance(result, dict): return False
        error_msg = result.get('error', '').lower()
        return 'token rate limit' in error_msg or 'rate limit' in error_msg

    def _extract_wait_time_from_error(self, result: Dict[str, Any]) -> float:
        headers = result.get('response_headers', {})
        if 'retry-after' in headers:
            try: return float(headers['retry-after'])
            except (ValueError, TypeError): pass
        error_msg = result.get('error', '').lower()
        match = re.search(r'try again in.*?(\d+\.?\d*)\s*s', error_msg)
        if match: return float(match.group(1))
        return 60.0

    @staticmethod
    def _normalize_ids(ids: Any, total_items: int) -> Optional[List[str]]:
        if ids is None: return None
        if hasattr(ids, 'tolist'): id_list = ids.tolist()
        elif hasattr(ids, 'values'): id_list = list(ids.values)
        elif not isinstance(ids, list): id_list = list(ids)
        else: id_list = ids
        if len(id_list) != total_items:
            raise ValueError(f"custom_ids deve ter o mesmo tamanho que texts: {len(id_list)} != {total_items}")
        return [str(cid) if cid is not None else None for cid in id_list]

    async def _make_api_call(self, messages: List[Dict[str, str]], json_schema: Optional[Dict[str, Any]],
                             request_id: str, batch_id: Optional[str] = None) -> Dict[str, Any]:
        self._validate_json_schema_compatibility(json_schema)
        await self.rate_limiter.await_permission_to_proceed()
        await self.stats_manager.record_concurrent_start(batch_id)
        
        try:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(MAX_RETRY), wait=wait_fixed(1), reraise=True):
                with attempt:
                    start_time = time.time()
                    try:
                        api_params = {"model": self.model, "messages": messages, "temperature": self.temperature}
                        if self.max_tokens: api_params["max_tokens"] = self.max_tokens
                        if json_schema: api_params["response_format"] = {"type": "json_object"}
                        response = await self.client.chat.completions.create(**api_params)
                        
                        input_tokens = response.usage.prompt_tokens
                        output_tokens = response.usage.completion_tokens
                        cached_tokens = getattr(getattr(response.usage, 'prompt_tokens_details', {}), 'cached_tokens', 0)
                        
                        content = response.choices[0].message.content
                        parsed_content = content
                        if json_schema and content:
                            try: parsed_content = json.loads(content)
                            except json.JSONDecodeError: pass
                        
                        return {
                            'id': request_id, 'success': True, 'content': parsed_content,
                            'input_tokens': input_tokens, 'output_tokens': output_tokens,
                            'cached_tokens': cached_tokens, 'total_tokens': response.usage.total_tokens,
                            'cost': self._calculate_cost(input_tokens, output_tokens, cached_tokens),
                            'api_response_time': time.time() - start_time, 'attempts': attempt.retry_state.attempt_number,
                            'raw_content': content
                        }
                    except Exception as e:
                        error_details = {'type': type(e).__name__, 'message': str(e), 'traceback': traceback.format_exc()}
                        error_result = {
                            'id': request_id, 'success': False, 'error': str(e), 'error_details': error_details,
                            'error_type': type(e).__name__, 'api_response_time': time.time() - start_time,
                            'attempts': attempt.retry_state.attempt_number,
                            'response_headers': dict(getattr(e.response, 'headers', {}) if hasattr(e, 'response') else {}),
                        }
                        if self._is_rate_limit_error(error_result):
                            wait_time = self._extract_wait_time_from_error(error_result)
                            self.rate_limiter.record_api_rate_limit(wait_time)
                            await self.stats_manager.record_rate_limiter_event(
                                {'event_type': 'api_rate_limit_detected', 'wait_time': wait_time}, batch_id)
                            raise
                        return error_result
        except RetryError as retry_error:
            last_exception = retry_error.last_attempt.exception()
            error_details = {'type': type(last_exception).__name__, 'message': str(last_exception), 'traceback': traceback.format_exc()}
            return {
                'id': request_id, 'success': False, 'error': f'MÃ¡ximo de {MAX_RETRY} tentativas excedido.',
                'error_details': error_details, 'error_type': 'RetryError', 'attempts': MAX_RETRY,
            }
        finally:
            await self.stats_manager.record_concurrent_end(batch_id)
    
    def _create_ordered_result(self, result: Dict[str, Any], start_timestamp_iso: str) -> Dict[str, Any]:
        """Cria um dicionÃ¡rio de resultado final com a ordem de campos especificada."""
        if result.get('success'):
            return {
                'id': result.get('id'), 'start_timestamp': start_timestamp_iso,
                'success': result.get('success'), 'api_response_time': result.get('api_response_time'),
                'attempts': result.get('attempts'), 'input_tokens': result.get('input_tokens'),
                'output_tokens': result.get('output_tokens'), 'cached_tokens': result.get('cached_tokens'),
                'total_tokens': result.get('total_tokens'), 'cost': result.get('cost'),
                'raw_content': result.get('raw_content'), 'content': result.get('content'),
            }
        else:
            return {
                'id': result.get('id'), 'start_timestamp': start_timestamp_iso,
                'success': result.get('success'), 'error': result.get('error'),
                'error_details': result.get('error_details'),
                'api_response_time': result.get('api_response_time'), 'attempts': result.get('attempts'),
            }
            
    async def process_single(self, text: str, prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                             custom_id: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """Processa um Ãºnico texto, retornando um resultado com estrutura ordenada."""
        start_time_ts = time.time()
        start_timestamp_iso = datetime.fromtimestamp(start_time_ts, tz=timezone.utc).astimezone(timezone(timedelta(hours=-3))).strftime('%Y-%m-%d %H:%M:%S')
        
        request_id = custom_id or f"req_{int(start_time_ts * 1000)}"
        prompt = prompt_template.format(text=text, **kwargs)
        messages = [{"role": "user", "content": prompt}]
        
        result = await self._make_api_call(messages, json_schema, request_id)
        
        self.rate_limiter.record_request_completion(
            tokens_used=result.get('total_tokens', 0), success=result.get('success', False))
        
        await self.stats_manager.record_request(
            model=self.model, retry_count=max(0, result.get('attempts', 1) - 1), **result)
        
        return self._create_ordered_result(result, start_timestamp_iso)

    async def _process_item_in_batch(self, item_data, prompt_template, json_schema, batch_id, progress_tracker, **kwargs):
        """FunÃ§Ã£o worker que executa uma tarefa e retorna um resultado ordenado."""
        start_time_ts = time.time()
        start_timestamp_iso = datetime.fromtimestamp(start_time_ts, tz=timezone.utc).astimezone(timezone(timedelta(hours=-3))).strftime('%Y-%m-%d %H:%M:%S')
        result = {}
        try:
            index, text, custom_id = item_data
            request_id = custom_id or f"{batch_id}_req_{index}"
            prompt = prompt_template.format(text=text, **kwargs)
            messages = [{"role": "user", "content": prompt}]
            
            result = await self._make_api_call(messages, json_schema, request_id, batch_id)
            
            self.rate_limiter.record_request_completion(
                tokens_used=result.get('total_tokens', 0), success=result.get('success', False))
            
            await self.stats_manager.record_request(
                batch_id=batch_id, model=self.model, retry_count=max(0, result.get('attempts', 1) - 1), **result)
            
            return self._create_ordered_result(result, start_timestamp_iso)
        finally:
            await progress_tracker.increment_and_log()

    async def process_batch(self, texts: List[str], prompt_template: str, json_schema: Optional[Dict[str, Any]] = None,
                            batch_id: Optional[str] = None, custom_ids: Optional[Any] = None, **kwargs) -> Dict[str, Any]:
        """Processa uma lista de textos em lote com logs de progresso e erros detalhados."""
        unique_timestamp = int(time.time())
        if batch_id:
            batch_id = f"{batch_id}_{unique_timestamp}"
        else:
            batch_id = f"batch_{unique_timestamp}"

        self.stats_manager.start_batch(batch_id)
        normalized_ids = self._normalize_ids(custom_ids, len(texts))
        
        logger.info(
            f"ðŸš€ Iniciando processamento em lote '{batch_id}' com {len(texts)} itens.",
            extra={'action': 'batch_start', 'batch_id': batch_id, 'total_items': len(texts)}
        )
        
        progress_tracker = self._BatchProgressTracker(total=len(texts), batch_id=batch_id)
        
        items_to_process = [
            (i, texts[i], normalized_ids[i] if normalized_ids else None)
            for i in range(len(texts))
        ]

        tasks = [
            asyncio.create_task(self._process_item_in_batch(item, prompt_template, json_schema, batch_id, progress_tracker, **kwargs))
            for item in items_to_process
        ]
        
        results = await asyncio.gather(*tasks)
        
        batch_stats = self.stats_manager.end_batch(batch_id)
        
        log_extra = {
            'action': 'batch_complete', 'batch_id': batch_id,
            'successful_requests': batch_stats.successful_requests,
            'failed_requests': batch_stats.failed_requests,
            'total_requests': batch_stats.total_requests,
            'processing_time_seconds': round(batch_stats.processing_time, 2),
            'start_time_iso': batch_stats.start_time_brt,
            'end_time_iso': batch_stats.end_time_brt,
            'total_cost': round(batch_stats.total_cost, 4)
        }
        logger.info(
            f"âœ… Lote '{batch_id}' concluÃ­do em {log_extra['processing_time_seconds']:.2f}s. "
            f"Sucesso: {batch_stats.successful_requests}, Falhas: {batch_stats.failed_requests}.",
            extra=log_extra
        )
        
        return {'results': results, 'batch_stats': batch_stats, 'batch_id': batch_id}

    def get_stats_manager(self) -> StatsManager:
        """Retorna a instÃ¢ncia do StatsManager para anÃ¡lise externa."""
        return self.stats_manager
