import asyncio
import time
import logging
from collections import deque
from typing import Deque, Tuple, Callable, Dict, Any

# Configura o logger para este módulo
logger = logging.getLogger(__name__)

# --- Constantes de Configuração para o Rate Limiter Dinâmico ---

# Período da janela deslizante para cálculo de TPM (Tokens Por Minuto)
WINDOW_SECONDS = 60.0

# Número de requisições recentes a serem usadas para calcular o custo médio
REQUEST_COST_HISTORY_SIZE = 50

# Custo em tokens a ser assumido para a primeira requisição, antes de termos dados reais
DEFAULT_PREDICTED_COST = 1000

# Limites para o ajuste dinâmico de concorrência
INITIAL_CONCURRENCY = 10  # Aumentado conforme solicitado
MIN_CONCURRENCY = 1
MAX_CONCURRENCY = 50 # Um limite superior seguro para evitar sobrecarga

# Frequência com que a concorrência é avaliada para ajuste (após N requisições)
CONCURRENCY_ADJUSTMENT_THRESHOLD = 10 # Reduzido para ajustes mais frequentes


class AdaptiveRateLimiter:
    """
    Controla o fluxo de requisições à API de forma dinâmica e preditiva.

    Esta classe gerencia o throughput para operar próximo ao limite máximo de
    Tokens Por Minuto (TPM) sem excedê-lo. Ela utiliza uma abordagem reativa
    e preditiva em vez de estimativas prévias.
    """

    def __init__(
        self,
        max_tpm: int,
        stats_callback: Callable[[Dict[str, Any]], None],
    ):
        """
        Inicializa o Rate Limiter dinâmico.
        """
        if max_tpm <= 0:
            raise ValueError("max_tpm deve ser um valor positivo.")
        
        self.max_tpm = max_tpm
        self._stats_callback = stats_callback
        self._lock = asyncio.Lock()

        # Estruturas de dados para controle de fluxo
        self.token_usage_window: Deque[Tuple[float, int]] = deque()
        self.tokens_in_window: int = 0
        self.recent_request_costs: Deque[int] = deque(maxlen=REQUEST_COST_HISTORY_SIZE)
        
        # Estado do controle dinâmico
        self._avg_request_cost: float = DEFAULT_PREDICTED_COST
        self._global_pause_until: float = 0.0
        self.dynamic_concurrency = INITIAL_CONCURRENCY
        self._successful_reqs_since_adjustment: int = 0

        # O semáforo que controla a concorrência. É recriado quando a concorrência muda.
        self._semaphore = asyncio.Semaphore(self.dynamic_concurrency)

        logger.info(
            "AdaptiveRateLimiter inicializado com estratégia de controle dinâmico",
            extra={
                'action': 'rate_limiter_init',
                'strategy': 'dynamic_concurrency_token_budget',
                'max_tpm': self.max_tpm,
                'initial_concurrency': self.dynamic_concurrency,
            },
        )

    def _prune_usage_window(self) -> None:
        """Remove registros de tokens da janela que são mais antigos que WINDOW_SECONDS."""
        now = time.time()
        while self.token_usage_window and (now - self.token_usage_window[0][0] > WINDOW_SECONDS):
            _timestamp, tokens = self.token_usage_window.popleft()
            self.tokens_in_window -= tokens

    def _update_semaphore(self):
        """Recria o semáforo com o novo valor de concorrência."""
        self._semaphore = asyncio.Semaphore(self.dynamic_concurrency)
        logger.info(
            f"Nível de concorrência ajustado para {self.dynamic_concurrency}",
            extra={
                'action': 'concurrency_adjusted',
                'new_concurrency': self.dynamic_concurrency,
                'current_tpm_rate': self.tokens_in_window
            }
        )
        self._stats_callback({
            'event_type': 'concurrency_update',
            'new_concurrency': self.dynamic_concurrency
        })

    async def _adjust_concurrency(self, is_success: bool = False, is_rate_limit_error: bool = False):
        """Ajusta o nível de concorrência de forma inteligente e agressiva."""
        current_concurrency = self.dynamic_concurrency

        if is_rate_limit_error:
            # Reação agressiva a um rate limit: reduzir pela metade ou para o mínimo.
            self.dynamic_concurrency = max(MIN_CONCURRENCY, int(self.dynamic_concurrency / 2))
            self._successful_reqs_since_adjustment = 0 # Reseta o contador
        elif is_success:
            self._successful_reqs_since_adjustment += 1
            # A cada N requisições, reavalia a concorrência ideal
            if self._successful_reqs_since_adjustment >= CONCURRENCY_ADJUSTMENT_THRESHOLD:
                if self._avg_request_cost > 0:
                    # Calcula a concorrência máxima teórica para não exceder o TPM
                    # Este é um cálculo aproximado que assume que as requisições duram menos de um minuto
                    # e que o custo é o principal fator limitante.
                    # Ex: 125.000 TPM / 1000 tokens/req = 125 reqs por minuto. 
                    # A concorrência ideal depende da duração de cada request. 
                    # Usamos um fator de segurança para não ser excessivamente agressivo.
                    # O cálculo mais simples é limitar o número de requests por minuto.
                    # Se um request leva 2s, um "worker" faz 30/minuto.
                    # Vamos usar um cálculo mais direto e empírico.
                    tpm_per_request_per_minute = self._avg_request_cost # Assumindo que a duração é < 1 min
                    
                    # Target é 85% do TPM para ter margem de segurança.
                    target_tpm = self.max_tpm * 0.85
                    # Concorrência ideal = target_tpm / (tpm por request)
                    max_possible_concurrency = int(target_tpm / tpm_per_request_per_minute)
                    
                    # Garante que o valor está dentro dos limites seguros
                    max_possible_concurrency = max(MIN_CONCURRENCY, min(MAX_CONCURRENCY, max_possible_concurrency))
                    
                    # Move a concorrência em direção ao valor ideal
                    if self.dynamic_concurrency < max_possible_concurrency:
                        self.dynamic_concurrency = min(self.dynamic_concurrency + 2, max_possible_concurrency)
                    elif self.dynamic_concurrency > max_possible_concurrency:
                        self.dynamic_concurrency = max(max_possible_concurrency, self.dynamic_concurrency - 2)
                else: # Se não temos custo médio, aumenta conservadoramente
                    self.dynamic_concurrency = min(self.dynamic_concurrency + 1, MAX_CONCURRENCY)
                
                self._successful_reqs_since_adjustment = 0
        else: # Falha genérica (não rate limit)
            # Redução moderada para falhas não relacionadas a rate limit.
            self.dynamic_concurrency = max(MIN_CONCURRENCY, self.dynamic_concurrency - 1)
            self._successful_reqs_since_adjustment = 0

        if current_concurrency != self.dynamic_concurrency:
            self._update_semaphore()

    async def await_permission_to_proceed(self) -> None:
        """Ponto de entrada que aguarda permissão para executar, respeitando concorrência e TPM."""
        await self._semaphore.acquire()

        async with self._lock:
            # Verifica se há uma pausa global forçada por um erro de rate limit
            now = time.time()
            if now < self._global_pause_until:
                wait_time = self._global_pause_until - now
                await asyncio.sleep(wait_time)

            # Executa a pausa proativa baseada no orçamento de tokens
            self._prune_usage_window()
            predicted_cost = self._avg_request_cost
            available_tpm = self.max_tpm - self.tokens_in_window

            # **CORREÇÃO ANTI-STALL**: Só tenta pausar se tiver registros na janela de tempo
            if predicted_cost > available_tpm and self.token_usage_window:
                # Calcula o tempo necessário para que o token mais antigo saia da janela
                wait_time = (self.token_usage_window[0][0] + WINDOW_SECONDS) - time.time()
                wait_time = max(wait_time, 0.1) # Espera mínima para evitar loops
                
                logger.info(
                    f"Pausa proativa para evitar rate limit. Aguardando {wait_time:.2f}s.",
                    extra={'action': 'proactive_pause', 'wait_time': wait_time}
                )
                self._stats_callback({'event_type': 'proactive_pause', 'wait_time': wait_time})
                await asyncio.sleep(wait_time)

    def record_request_completion(self, tokens_used: int, success: bool):
        """Registra o resultado de uma requisição para atualizar o estado do limiter."""
        try:
            self._semaphore.release()
        except ValueError:
            # Pode acontecer se o semáforo for recriado enquanto uma tarefa libera. Seguro ignorar.
            pass
        asyncio.create_task(self._async_record_request(tokens_used, success))

    async def _async_record_request(self, tokens_used: int, success: bool):
        """Processa o registro de forma assíncrona para não bloquear o chamador."""
        async with self._lock:
            if success and tokens_used > 0:
                now = time.time()
                self.token_usage_window.append((now, tokens_used))
                self.tokens_in_window += tokens_used

                self.recent_request_costs.append(tokens_used)
                self._avg_request_cost = sum(self.recent_request_costs) / len(self.recent_request_costs)
            
            await self._adjust_concurrency(is_success=success)

    def record_api_rate_limit(self, wait_time: float):
        """Ativa o "freio de emergência" quando um erro de rate limit da API é detectado."""
        try:
            self._semaphore.release()
        except ValueError:
            pass
        asyncio.create_task(self._async_record_rate_limit(wait_time))

    async def _async_record_rate_limit(self, wait_time: float):
        """Processa o registro de rate limit de forma assíncrona."""
        async with self._lock:
            self._global_pause_until = time.time() + wait_time
            await self._adjust_concurrency(is_rate_limit_error=True)
            logger.error(
                f"RATE LIMIT DA API DETECTADO! Pausa global por {wait_time:.2f}s, concorrência reduzida para {self.dynamic_concurrency}.",
                extra={'action': 'api_rate_limit_detected', 'wait_time': wait_time}
            )
